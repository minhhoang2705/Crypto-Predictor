# Cryptocurrency Price Prediction MLOps Project üöÄ

[![Python](https://img.shields.io/badge/Python-3.9-blue.svg)](https://www.python.org/downloads/release/python-390/)
[![MLflow](https://img.shields.io/badge/MLflow-2.7.1-brightgreen.svg)](https://mlflow.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-0.103.1-green.svg)](https://fastapi.tiangolo.com/)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.27.0-red.svg)](https://streamlit.io/)
[![Code style: black](https://img.shields.io/badge/code%20style-black-000000.svg)](https://github.com/psf/black)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

A real-time cryptocurrency price prediction system with continuous training and deployment capabilities. This project implements a complete MLOps pipeline with automated data collection, model training, and real-time predictions.

## üåü Table of Contents
- [Features](#-features)
- [Architecture](#Ô∏è-architecture)
- [Quick Start](#-quick-start)
- [Technical Details](#-technical-details)
- [API Reference](#-api-reference)
- [Performance](#-performance)
- [Troubleshooting](#-troubleshooting)
- [Development](#Ô∏è-development)
- [Contributing](#-contributing)

## üåü Features

- **Real-time Data Collection**: Automated collection of cryptocurrency data from Binance
- **Advanced ML Pipeline**: XGBoost model with comprehensive feature engineering
- **Interactive Dashboard**: Real-time price charts and predictions visualization
- **Automated Training**: Continuous model training with performance tracking
- **MLOps Integration**: Full MLflow integration for experiment tracking
- **API Service**: FastAPI-based prediction service
- **Monitoring**: Prometheus metrics integration

## üèóÔ∏è Architecture

```
.
‚îú‚îÄ‚îÄ data/                  # Data storage
‚îÇ   ‚îú‚îÄ‚îÄ raw/              # Raw data from Binance
‚îÇ   ‚îî‚îÄ‚îÄ processed/        # Processed datasets
‚îú‚îÄ‚îÄ models/               # Trained models
‚îú‚îÄ‚îÄ src/                  # Source code
‚îÇ   ‚îú‚îÄ‚îÄ data/            # Data collection and processing
‚îÇ   ‚îú‚îÄ‚îÄ models/          # Model training and evaluation
‚îÇ   ‚îú‚îÄ‚îÄ api/             # FastAPI service
‚îÇ   ‚îú‚îÄ‚îÄ ui/              # Streamlit dashboard
‚îÇ   ‚îî‚îÄ‚îÄ monitoring/      # Monitoring and logging
‚îú‚îÄ‚îÄ tests/               # Unit and integration tests
‚îî‚îÄ‚îÄ configs/             # Configuration files
```

## üöÄ Quick Start

### Prerequisites

- Python 3.9+
- Conda package manager
- Binance API credentials

### Installation

1. Clone the repository:
```bash
git clone https://github.com/minhhoang2705/crypto-prediction.git
cd crypto-prediction
```

2. Create and activate Conda environment:
```bash
conda env create -f conda_env.yml
conda activate crypto_prediction
```

3. Set up environment variables:
```bash
cp .env.example .env
# Edit .env with your Binance API credentials
```

### Running the Application

1. Start data collection:
```bash
python src/data/ingest.py
```

2. Start MLflow tracking server:
```bash
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

3. Train the model:
```bash
python src/models/train.py
```

4. Start the API server:
```bash
uvicorn src.api.main:app --host 0.0.0.0 --port 8000
```

5. Launch the dashboard:
```bash
streamlit run src/ui/app.py
```

## üìä Technical Details

### Model Architecture

#### XGBoost Configuration
```python
model_params = {
    'n_estimators': 200,
    'max_depth': 10,
    'learning_rate': 0.05,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'min_child_weight': 3,
    'objective': 'reg:squarederror',
    'random_state': 42
}
```

#### Feature Engineering Pipeline
1. **Price Indicators**
   - Returns: `r(t) = (price(t) - price(t-1)) / price(t-1)`
   - Log Returns: `log(1 + r(t))`
   - Rolling Statistics: Mean, Std, Min, Max

2. **Technical Indicators**
   - Moving Averages (SMA, EMA)
   ```python
   SMA = price.rolling(window=N).mean()
   EMA = price.ewm(span=N).mean()
   ```
   - RSI (14-period)
   - MACD (12, 26, 9)
   - Bollinger Bands (20, 2)

3. **Volume Indicators**
   - OBV (On-Balance Volume)
   - VWAP (Volume-Weighted Average Price)
   - Volume Profile

4. **Time-based Features**
   - Hour of Day (cyclical encoding)
   - Day of Week (one-hot encoding)
   - Market Session Indicators

### Training Pipeline

1. **Data Preprocessing**
   - Missing value imputation
   - Outlier detection (IQR method)
   - Feature scaling (StandardScaler)

2. **Cross-validation**
   - Time series split (5 folds)
   - Walk-forward optimization

3. **Model Training**
   - Early stopping (20 rounds)
   - Feature importance tracking
   - Model versioning

## üì° API Reference

### Authentication
```python
headers = {
    "Content-Type": "application/json",
    "Accept": "application/json",
}
```

### Make Predictions
```python
import requests

# Prediction endpoint
url = "http://localhost:8000/predict"

# Example data
data = {
    "open": 45000.0,
    "high": 45100.0,
    "low": 44900.0,
    "close": 45050.0,
    "volume": 100.0,
    "sma_5": 45000.0,
    "sma_20": 44800.0,
    "rsi": 55.0
}

# Make request
response = requests.post(url, json=data, headers=headers)
prediction = response.json()
print(f"Predicted Price: ${prediction['predicted_price']:.2f}")
print(f"Confidence: {prediction['confidence']:.2%}")
```

### Health Check
```python
response = requests.get("http://localhost:8000/health")
print(f"API Status: {response.json()['status']}")
```

## üìä Performance

### Model Benchmarks
| Metric | Value |
|--------|--------|
| MSE    | 0.00123 |
| RMSE   | 0.0351 |
| MAPE   | 1.23% |
| R¬≤     | 0.87 |

### Latency Metrics
| Endpoint | P50 | P95 | P99 |
|----------|-----|-----|-----|
| /predict | 45ms| 95ms| 125ms|
| /health  | 2ms | 5ms | 10ms |

### Resource Usage
- Memory: ~500MB
- CPU: ~20% (single core)
- Disk: ~100MB/day

## üîç Troubleshooting

### Common Issues

1. **Data Collection Issues**
```bash
Error: No data received from Binance
```
**Solution**:
- Check Binance API credentials
- Verify internet connection
- Ensure API rate limits haven't been exceeded
```bash
# Check API status
curl -i https://api.binance.com/api/v3/ping
```

2. **Model Training Failures**
```bash
Error: Could not find any data files
```
**Solution**:
- Ensure data collection is running
- Check data directory permissions
- Verify file paths in configuration

3. **API Connection Issues**
```bash
Error: Connection refused
```
**Solution**:
- Check if API server is running
- Verify port availability
```bash
# Check port usage
lsof -i :8000
# Kill process if needed
kill -9 $(lsof -t -i:8000)
```

4. **MLflow Tracking Issues**
```bash
Error: Could not connect to MLflow server
```
**Solution**:
- Start MLflow server
- Check database connection
- Verify tracking URI
```bash
# Start MLflow with SQLite
mlflow ui --backend-store-uri sqlite:///mlflow.db
```

### Performance Optimization

1. **Reduce Memory Usage**
```python
# Use efficient data types
df = df.astype({
    'open': 'float32',
    'volume': 'float32'
})
```

2. **Speed Up Predictions**
```python
# Cache feature engineering
@st.cache_data
def calculate_features(data):
    # Feature calculation
    return features
```

## üõ† Deployment Guides

### Local Deployment

```bash
# 1. Clone repository
git clone https://github.com/minhhoang2705/crypto-prediction.git
cd crypto-prediction

# 2. Create conda environment
conda env create -f conda_env.yml
conda activate crypto_prediction

# 3. Run services
python src/data/ingest.py &  # Start data collection
mlflow ui --backend-store-uri sqlite:///mlflow.db &  # Start MLflow
uvicorn src.api.main:app --host 0.0.0.0 --port 8000 &  # Start API
streamlit run src/ui/app.py  # Start dashboard
```

### Docker Deployment

```bash
# Build image
docker build -t crypto-prediction .

# Run container
docker run -d \
    -p 8000:8000 \
    -p 8501:8501 \
    -p 5000:5000 \
    -v $(pwd)/data:/app/data \
    -v $(pwd)/models:/app/models \
    --name crypto-prediction \
    crypto-prediction

# View logs
docker logs -f crypto-prediction
```

### AWS Deployment

1. **EC2 Setup**:
```bash
# Install required packages
sudo yum update -y
sudo yum install git docker -y
sudo service docker start
sudo usermod -a -G docker ec2-user

# Clone and deploy
git clone https://github.com/minhhoang2705/crypto-prediction.git
cd crypto-prediction
docker-compose up -d
```

2. **ECS Deployment**:
```bash
# Configure AWS CLI
aws configure

# Create ECR repository
aws ecr create-repository --repository-name crypto-prediction

# Push to ECR
aws ecr get-login-password --region region | docker login --username AWS --password-stdin aws_account_id.dkr.ecr.region.amazonaws.com
docker tag crypto-prediction:latest aws_account_id.dkr.ecr.region.amazonaws.com/crypto-prediction:latest
docker push aws_account_id.dkr.ecr.region.amazonaws.com/crypto-prediction:latest
```

### Google Cloud Platform

1. **Cloud Run**:
```bash
# Build container
gcloud builds submit --tag gcr.io/PROJECT_ID/crypto-prediction

# Deploy to Cloud Run
gcloud run deploy crypto-prediction \
    --image gcr.io/PROJECT_ID/crypto-prediction \
    --platform managed \
    --region us-central1 \
    --allow-unauthenticated
```

2. **GKE (Kubernetes)**:
```bash
# Create cluster
gcloud container clusters create crypto-cluster \
    --num-nodes=3 \
    --machine-type=e2-medium

# Deploy application
kubectl apply -f k8s/
```

### Azure Deployment

```bash
# Create Azure Container Registry
az acr create --name cryptoprediction --resource-group mygroup --sku Basic

# Build and push
az acr build --registry cryptoprediction --image crypto-prediction:latest .

# Deploy to App Service
az webapp create \
    --resource-group mygroup \
    --plan myplan \
    --name crypto-prediction \
    --deployment-container-image-name cryptoprediction.azurecr.io/crypto-prediction:latest
```

## ‚ö° Advanced Performance Optimization

### 1. Data Pipeline Optimization

```python
# 1. Efficient data reading
def read_data_efficiently():
    # Use dtype specification
    dtypes = {
        'timestamp': 'datetime64[ns]',
        'open': 'float32',
        'high': 'float32',
        'low': 'float32',
        'close': 'float32',
        'volume': 'float32'
    }
    
    # Use chunking for large files
    chunks = pd.read_csv('data.csv', dtype=dtypes, chunksize=10000)
    return pd.concat(chunks)

# 2. Parallel feature calculation
from concurrent.futures import ThreadPoolExecutor

def parallel_feature_engineering(df):
    with ThreadPoolExecutor() as executor:
        future_sma = executor.submit(calculate_sma, df)
        future_ema = executor.submit(calculate_ema, df)
        future_rsi = executor.submit(calculate_rsi, df)
    
    return pd.concat([
        future_sma.result(),
        future_ema.result(),
        future_rsi.result()
    ], axis=1)
```

### 2. Model Optimization

```python
# 1. Feature selection
from sklearn.feature_selection import SelectFromModel

def optimize_features(X, y):
    selector = SelectFromModel(
        estimator=XGBRegressor(importance_type='gain'),
        prefit=False,
        threshold='median'
    )
    return selector.fit_transform(X, y)

# 2. Model compression
def compress_model(model):
    # Prune low-importance features
    model.save_model('temp.json')
    model = xgb.Booster()
    model.load_model('temp.json')
    model.set_param({'predictor': 'cpu_predictor'})
    return model
```

### 3. API Performance

```python
# 1. Response caching
from fastapi_cache import FastAPICache
from fastapi_cache.backends.redis import RedisBackend

@app.on_event("startup")
async def startup():
    FastAPICache.init(RedisBackend(), prefix="fastapi-cache")

@app.get("/cached-prediction")
@cache(expire=30)  # Cache for 30 seconds
async def get_cached_prediction():
    return compute_expensive_prediction()

# 2. Batch predictions
@app.post("/batch-predict")
async def batch_predict(data: List[PredictionInput]):
    predictions = []
    for batch in create_batches(data, batch_size=32):
        batch_predictions = model.predict(batch)
        predictions.extend(batch_predictions)
    return predictions
```

### 4. Memory Management

```python
# 1. Generator for large datasets
def data_generator(file_pattern):
    for file in glob.glob(file_pattern):
        data = pd.read_csv(file)
        yield process_chunk(data)
        del data
        gc.collect()

# 2. Memory-efficient feature engineering
def calculate_features_efficiently(df):
    # Use inplace operations
    df['returns'] = df['close'].pct_change(inplace=True)
    df['sma'] = df['close'].rolling(window=20).mean(engine='numba')
    return df
```

### 5. Dashboard Optimization

```python
# 1. Efficient data loading
@st.cache_data(ttl=300)  # Cache for 5 minutes
def load_dashboard_data():
    return pd.read_csv('latest_data.csv')

# 2. Optimize chart rendering
def optimize_chart(fig):
    fig.update_layout(
        uirevision=True,  # Preserve zoom level
        modebar_remove=['sendDataToCloud', 'select2d', 'lasso2d'],
        hovermode='x unified'
    )
    return fig
```

### 6. Production Monitoring

```python
# 1. Performance metrics
from prometheus_client import Histogram, Counter

PREDICTION_LATENCY = Histogram(
    'prediction_latency_seconds',
    'Time spent processing prediction',
    buckets=(0.01, 0.025, 0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1.0)
)

# 2. Resource monitoring
import psutil

def monitor_resources():
    return {
        'cpu_percent': psutil.cpu_percent(),
        'memory_percent': psutil.virtual_memory().percent,
        'disk_usage': psutil.disk_usage('/').percent
    }
```

## üõ†Ô∏è Development

### Running Tests
```bash
pytest tests/ --cov=src
```

### Code Style
```bash
black src/
flake8 src/
```

### Adding New Features
1. Create a new branch
2. Implement your feature
3. Add tests
4. Submit a pull request

## üìù License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ü§ù Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## üìß Contact

For questions and feedback, please open an issue or contact [tranhminh8464@gmail.com].

## üôè Acknowledgments

- Binance API for real-time cryptocurrency data
- MLflow for experiment tracking
- FastAPI for API development
- Streamlit for dashboard creation